\n# Supabase Migration Setup Guide\n\n## Quick Start: Running the Database Migration\n\nFollow these steps to set up your Supabase database for the Workshop Document Processor.\n\n### Step 1: Access Supabase SQL Editor\n\n1. Go to **https://supabase.com** and sign in\n2. Open your project: `workshop-document-processor` \n3. In the left sidebar, click on **SQL Editor**\n4. Click the **New query** button\n\n### Step 2: Copy the Migration Script\n\nCopy the entire content from `backend/supabase/migrations/001_create_documents_table.sql`:\n\n```sql\n-- Create documents table for tracking document processing jobs\nCREATE TABLE IF NOT EXISTS documents (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    filename TEXT NOT NULL,\n    status TEXT NOT NULL CHECK (status IN ('queued', 'processing', 'complete', 'failed')),\n    processing_options JSONB,\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n    completed_at TIMESTAMP WITH TIME ZONE,\n    error_message TEXT,\n    file_size BIGINT,\n    content_type TEXT,\n    processed_file_path TEXT,\n    original_file_path TEXT\n);\n\n-- Create indexes for better query performance\nCREATE INDEX IF NOT EXISTS idx_documents_status ON documents(status);\nCREATE INDEX IF NOT EXISTS idx_documents_created_at ON documents(created_at);\nCREATE INDEX IF NOT EXISTS idx_documents_filename ON documents(filename);\n\n-- Create storage buckets for file uploads\nINSERT INTO storage.buckets (id, name, public, file_size_limit, allowed_mime_types)\nVALUES (\n    'uploads',\n    'uploads',\n    false,\n    10485760, -- 10MB\n    ARRAY[\n        'application/pdf',\n        'application/vnd.openxmlformats-officedocument.wordprocessingml.document',\n        'application/vnd.openxmlformats-officedocument.presentationml.presentation',\n        'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n    ]\n) ON CONFLICT (id) DO NOTHING;\n\nINSERT INTO storage.buckets (id, name, public, file_size_limit, allowed_mime_types)\nVALUES (\n    'processed',\n    'processed',\n    false,\n    10485760, -- 10MB\n    ARRAY['text/markdown', 'text/plain']\n) ON CONFLICT (id) DO NOTHING;\n\n-- Enable Row Level Security\nALTER TABLE documents ENABLE ROW LEVEL SECURITY;\n\n-- Create RLS policies for documents table\n-- Allow all operations for now (since no authentication required for workshop)\nCREATE POLICY \"Allow all operations on documents\" ON documents\n    FOR ALL USING (true)\n    WITH CHECK (true);\n\n-- Create RLS policies for storage buckets\n-- Allow all operations on uploads bucket\nCREATE POLICY \"Allow all operations on uploads bucket\" ON storage.objects\n    FOR ALL USING (bucket_id = 'uploads')\n    WITH CHECK (bucket_id = 'uploads');\n\n-- Allow all operations on processed bucket\nCREATE POLICY \"Allow all operations on processed bucket\" ON storage.objects\n    FOR ALL USING (bucket_id = 'processed')\n    WITH CHECK (bucket_id = 'processed');\n\n-- Create function to update completed_at timestamp\nCREATE OR REPLACE FUNCTION update_completed_at()\nRETURNS TRIGGER AS $$\nBEGIN\n    IF NEW.status = 'complete' AND OLD.status != 'complete' THEN\n        NEW.completed_at = NOW();\n    END IF;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Create trigger to automatically update completed_at\nCREATE TRIGGER trigger_update_completed_at\n    BEFORE UPDATE ON documents\n    FOR EACH ROW\n    EXECUTE FUNCTION update_completed_at();\n\n-- Create view for document statistics\nCREATE OR REPLACE VIEW document_stats AS\nSELECT \n    status,\n    COUNT(*) as count,\n    AVG(EXTRACT(EPOCH FROM (completed_at - created_at))) as avg_processing_time_seconds\nFROM documents \nWHERE created_at >= NOW() - INTERVAL '24 hours'\nGROUP BY status;\n\n-- Grant permissions\nGRANT ALL ON documents TO authenticated;\nGRANT ALL ON documents TO anon;\nGRANT ALL ON storage.objects TO authenticated;\nGRANT ALL ON storage.objects TO anon;\nGRANT SELECT ON document_stats TO authenticated;\nGRANT SELECT ON document_stats TO anon;\n```\n\n### Step 3: Execute the Migration\n\n1. Paste the entire script into the SQL Editor\n2. Click the **Run** button (or press `Cmd/Ctrl + Enter`)\n3. Wait for the execution to complete (should take 2-3 seconds)\n\n### Step 4: Verify the Migration\n\nAfter running the script, verify that everything was created successfully:\n\n#### Check the Documents Table\n1. Go to **Table Editor** in the left sidebar\n2. You should see a new table called `documents`\n3. Click on it to see the columns:\n   - id (UUID, primary key)\n   - filename (TEXT)\n   - status (TEXT with check constraint)\n   - processing_options (JSONB)\n   - created_at (TIMESTAMP)\n   - completed_at (TIMESTAMP)\n   - error_message (TEXT)\n   - file_size (BIGINT)\n   - content_type (TEXT)\n   - processed_file_path (TEXT)\n   - original_file_path (TEXT)\n\n#### Check Storage Buckets\n1. Go to **Storage** in the left sidebar\n2. You should see two buckets:\n   - **uploads** (for original files)\n   - **processed** (for markdown output)\n3. Both should be marked as **Private**\n\n#### Check RLS Policies\n1. Go to **Authentication** → **Policies** in the left sidebar\n2. You should see policies for:\n   - `documents` table: \"Allow all operations on documents\"\n   - Storage `uploads` bucket: \"Allow all operations on uploads bucket\"\n   - Storage `processed` bucket: \"Allow all operations on processed bucket\"\n\n### Step 5: Test the Connection\n\n#### Option A: Using the Backend Health Check\n1. Open a terminal\n2. Run: `curl http://localhost:8000/api/health`\n3. You should see a JSON response like:\n```json\n{\n  \"status\": \"healthy\",\n  \"timestamp\": \"2025-10-04T14:00:00.000Z\",\n  \"version\": \"1.0.0\",\n  \"database_connected\": true,\n  \"storage_connected\": true\n}\n```\n\n#### Option B: Direct Database Query in Supabase\n1. Go back to **SQL Editor**\n2. Run this simple test query:\n```sql\nSELECT * FROM documents LIMIT 1;\n```\n3. Should return no results (empty table) with no errors\n\n### Step 6: End-to-End Smoke Test\n\n1. Start the frontend: Open http://localhost:3000 in your browser\n2. Upload a test PDF file (any small PDF < 10MB)\n3. Verify the workflow:\n   - File uploads successfully\n   - Processing status updates\n   - Can download the converted markdown file\n4. Check in Supabase:\n   - Go to **Table Editor** → `documents` table\n   - You should see a new record for your uploaded file\n   - Go to **Storage** → `uploads` bucket\n   - You should see your uploaded file\n   - Go to **Storage** → `processed` bucket\n   - You should see the converted markdown file\n\n## Troubleshooting\n\n### Error: \"relation 'storage.buckets' does not exist\"\n**Solution**: This means storage is not enabled. Go to **Storage** in Supabase dashboard and initialize it first.\n\n### Error: \"permission denied for table documents\"\n**Solution**: Make sure the RLS policies were created. Re-run the policy creation part of the migration.\n\n### Error: \"duplicate key value violates unique constraint\"\n**Solution**: The migration has already been run. This is safe to ignore, or you can remove the bucket creation if you want to re-run.\n\n### Backend shows \"database_connected: false\"\n**Solution**: \n1. Check that `SUPABASE_URL` and `SUPABASE_KEY` in `backend/.env` are correct\n2. Verify you're using the `service_role` key, not the `anon` key\n3. Check that the `documents` table exists in your Supabase project\n\n### Backend shows \"storage_connected: false\"\n**Solution**:\n1. Verify that storage is enabled in your Supabase project\n2. Check that the `uploads` and `processed` bu