# Story 2.6: Integration Testing & Error Scenario Validation

## Status
Draft

## Story
**As a** developer,
**I want** comprehensive integration tests covering success and failure paths,
**so that** we have confidence the system handles real-world scenarios reliably.

## Acceptance Criteria

1. Integration test suite created covering: successful upload → process → download for each file type (PDF, DOCX, PPTX, XLSX)
2. Processing options validated: Fast mode completes faster than Quality mode, OCR mode handles scanned PDF successfully
3. Error scenarios tested: oversized file rejection (11MB file), unsupported format rejection (.txt file), corrupted file handling (intentionally corrupted PDF)
4. Timeout scenario tested: mock Docling processing exceeding 5 minutes triggers failure status with correct error message
5. Storage failure scenarios: Supabase connection error handled gracefully with user-friendly error message
6. End-to-end test on staging environment: upload real workshop sample documents (complex PDF with tables, multi-slide PPTX, large Excel spreadsheet)
7. Test results documented: success rate, processing times (p50, p95), identified edge cases requiring handling in Epic 3

## Tasks / Subtasks

- [ ] Create integration test framework setup (AC: All)
  - [ ] Set up test environment configuration for integration tests
  - [ ] Create test data fixtures for various file types and scenarios
  - [ ] Implement test database isolation and cleanup
  - [ ] Configure mock Supabase and Docling services for testing
  - [ ] Set up test reporting and documentation generation

- [ ] Implement successful workflow integration tests (AC: 1)
  - [ ] Create `backend/tests/test_integration/test_complete_workflow.py`
  - [ ] Test PDF upload → processing → download workflow
  - [ ] Test DOCX upload → processing → download workflow
  - [ ] Test PPTX upload → processing → download workflow
  - [ ] Test XLSX upload → processing → download workflow
  - [ ] Verify file integrity and content preservation
  - [ ] Measure and document processing times for each file type

- [ ] Create processing options validation tests (AC: 2)
  - [ ] Create `backend/tests/test_integration/test_processing_options_integration.py`
  - [ ] Test Fast mode processing speed vs Quality mode
  - [ ] Test OCR functionality with scanned PDF documents
  - [ ] Test all processing option combinations (Fast/Quality × OCR on/off)
  - [ ] Verify processing output quality differences between modes
  - [ ] Document processing time differences and quality trade-offs

- [ ] Implement error scenario integration tests (AC: 3)
  - [ ] Create `backend/tests/test_integration/test_error_scenarios.py`
  - [ ] Test oversized file rejection (11MB+ files)
  - [ ] Test unsupported format rejection (.txt, .jpg, etc.)
  - [ ] Test corrupted file handling and error reporting
  - [ ] Verify error messages are user-friendly and actionable
  - [ ] Test error state cleanup and recovery

- [ ] Create timeout scenario testing (AC: 4)
  - [ ] Create `backend/tests/test_integration/test_timeout_scenarios.py`
  - [ ] Mock Docling processing to exceed 5-minute timeout
  - [ ] Verify timeout detection and error handling
  - [ ] Test timeout error message generation
  - [ ] Verify system cleanup after timeout scenarios
  - [ ] Test retry functionality after timeout errors

- [ ] Implement storage failure testing (AC: 5)
  - [ ] Create `backend/tests/test_integration/test_storage_failures.py`
  - [ ] Mock Supabase connection failures
  - [ ] Test upload storage failure handling
  - [ ] Test download storage failure handling
  - [ ] Verify graceful degradation and user-friendly error messages
  - [ ] Test storage retry logic and recovery mechanisms

- [ ] Create end-to-end staging environment tests (AC: 6)
  - [ ] Create `backend/tests/test_integration/test_staging_e2e.py`
  - [ ] Set up staging environment test configuration
  - [ ] Test with real workshop sample documents
  - [ ] Test complex PDF with tables and formatting
  - [ ] Test multi-slide PowerPoint presentations
  - [ ] Test large Excel spreadsheets with multiple sheets
  - [ ] Document real-world performance characteristics

- [ ] Implement test results documentation (AC: 7)
  - [ ] Create `backend/tests/test_integration/test_results_documentation.py`
  - [ ] Generate success rate reports for all test scenarios
  - [ ] Calculate and document processing time percentiles (p50, p95)
  - [ ] Identify and document edge cases and limitations
  - [ ] Create recommendations for Epic 3 improvements
  - [ ] Generate comprehensive test coverage report

- [ ] Add performance benchmarking (AC: 2, 7)
  - [ ] Create `backend/tests/test_integration/test_performance_benchmarks.py`
  - [ ] Benchmark processing times across different file sizes
  - [ ] Measure memory usage during processing
  - [ ] Test concurrent processing scenarios
  - [ ] Document performance bottlenecks and optimization opportunities
  - [ ] Create performance regression detection

- [ ] Implement cross-browser compatibility tests (AC: 6)
  - [ ] Create `frontend/tests/integration/test_cross_browser.py`
  - [ ] Test upload functionality across different browsers
  - [ ] Test status polling compatibility
  - [ ] Test download functionality across browsers
  - [ ] Verify mobile browser compatibility
  - [ ] Document browser-specific issues or workarounds

- [ ] Create test automation and CI/CD integration (AC: All)
  - [ ] Set up automated test execution in CI/CD pipeline
  - [ ] Configure test environment provisioning
  - [ ] Implement test result reporting and notifications
  - [ ] Create test data management and versioning
  - [ ] Set up test performance monitoring and alerting

## Dev Notes

### Previous Story Insights
[Source: Story 2.5 - Mobile Responsive UI Refinements]
- Mobile responsive components implemented and tested
- Cross-device compatibility verified
- Touch interactions optimized for mobile devices
- Performance optimizations applied for mobile bandwidth

[Source: Story 2.4 - Download Experience & File Management]
- Download functionality fully implemented with filename cleaning
- Success screen with download button working
- Error handling for download failures implemented
- File streaming from Supabase storage working

[Source: Story 2.3 - Comprehensive Error Handling & Actionable Messages]
- Error handling system implemented across all components
- User-friendly error messages with actionable guidance
- Error logging and debugging infrastructure in place
- Error state management and recovery working

[Source: Story 2.2 - Enhanced Status Display & Progress Indicators]
- Status polling system implemented with real-time updates
- Progress indicators with time estimation working
- Mobile-responsive status display implemented
- Terminal state detection and cleanup working

[Source: Story 2.1 - Processing Options UI & Backend Integration]
- Processing options UI with OCR and mode selection working
- Backend integration with Docling configuration implemented
- Processing time estimates established and validated
- All processing option combinations tested

### Integration Testing Architecture
[Source: docs/architecture.md#Testing-Strategy]

**Test Environment Setup:**
```python
# Integration test configuration
class IntegrationTestConfig:
    # Test database configuration
    TEST_DATABASE_URL = "postgresql://test_user:test_pass@localhost:5432/test_docling"
    
    # Mock service configuration
    MOCK_SUPABASE = True
    MOCK_DOCLING = True
    
    # Test file paths
    TEST_FIXTURES_DIR = "backend/tests/fixtures/integration/"
    
    # Performance thresholds
    MAX_PROCESSING_TIME_FAST = 60  # seconds
    MAX_PROCESSING_TIME_QUALITY = 180  # seconds
    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
    
    # Success rate thresholds
    MIN_SUCCESS_RATE = 0.95  # 95%
    MAX_ERROR_RATE = 0.05  # 5%
```

**Test Data Management:**
```python
# Test fixture management
class TestFixtureManager:
    def __init__(self):
        self.fixtures = {
            "pdf_simple": "simple_document.pdf",
            "pdf_complex": "complex_document_with_tables.pdf",
            "pdf_scanned": "scanned_document.pdf",
            "docx_simple": "simple_document.docx",
            "docx_complex": "complex_document.docx",
            "pptx_simple": "simple_presentation.pptx",
            "pptx_complex": "complex_presentation.pptx",
            "xlsx_simple": "simple_spreadsheet.xlsx",
            "xlsx_complex": "complex_spreadsheet.xlsx",
            "oversized": "oversized_document.pdf",  # 11MB+
            "corrupted": "corrupted_document.pdf",
            "unsupported": "unsupported_file.txt"
        }
    
    def get_fixture_path(self, fixture_name: str) -> str:
        return os.path.join(self.TEST_FIXTURES_DIR, self.fixtures[fixture_name])
```

### Complete Workflow Testing
[Source: docs/architecture.md#Integration-Testing]

**End-to-End Workflow Test:**
```python
@pytest.mark.integration
class TestCompleteWorkflow:
    async def test_pdf_workflow_success(self, client, test_db):
        """Test complete PDF upload → process → download workflow."""
        # 1. Upload file
        with open("test_fixtures/simple_document.pdf", "rb") as f:
            upload_response = client.post(
                "/api/upload",
                files={"file": ("test.pdf", f, "application/pdf")},
                data={"ocr_enabled": "false", "processing_mode": "fast"}
            )
        
        assert upload_response.status_code == 200
        document_id = upload_response.json()["id"]
        
        # 2. Monitor processing
        max_wait_time = 60  # seconds
        start_time = time.time()
        
        while time.time() - start_time < max_wait_time:
            status_response = client.get(f"/api/status/{document_id}")
            assert status_response.status_code == 200
            
            status = status_response.json()
            if status["status"] == "complete":
                break
            elif status["status"] == "failed":
                pytest.fail(f"Processing failed: {status.get('error_message')}")
            
            await asyncio.sleep(2)
        else:
            pytest.fail("Processing timed out")
        
        # 3. Download result
        download_response = client.get(f"/api/download/{document_id}")
        assert download_response.status_code == 200
        assert download_response.headers["content-type"] == "text/markdown"
        
        # 4. Verify content
        content = download_response.content.decode("utf-8")
        assert len(content) > 0
        assert "# " in content  # Markdown headers present
        
        # 5. Cleanup
        cleanup_response = client.delete(f"/api/documents/{document_id}")
        assert cleanup_response.status_code == 200
```

### Processing Options Validation
[Source: docs/architecture.md#Processing-Options]

**Processing Mode Comparison Test:**
```python
@pytest.mark.integration
class TestProcessingOptions:
    async def test_fast_vs_quality_mode_performance(self, client, test_db):
        """Compare processing time and quality between Fast and Quality modes."""
        test_file = "complex_document.pdf"
        
        # Test Fast mode
        fast_time = await self._measure_processing_time(
            client, test_file, processing_mode="fast"
        )
        
        # Test Quality mode
        quality_time = await self._measure_processing_time(
            client, test_file, processing_mode="quality"
        )
        
        # Verify Quality mode takes longer but produces better results
        assert quality_time > fast_time
        assert fast_time < 60  # Fast mode should complete within 60 seconds
        assert quality_time < 180  # Quality mode should complete within 3 minutes
        
        # Compare output quality (number of elements detected, accuracy, etc.)
        fast_quality = await self._measure_output_quality(client, test_file, "fast")
        quality_quality = await self._measure_output_quality(client, test_file, "quality")
        
        assert quality_quality >= fast_quality
    
    async def test_ocr_functionality(self, client, test_db):
        """Test OCR functionality with scanned documents."""
        scanned_pdf = "scanned_document.pdf"
        
        # Test without OCR
        result_no_ocr = await self._process_document(
            client, scanned_pdf, ocr_enabled=False
        )
        
        # Test with OCR
        result_with_ocr = await self._process_document(
            client, scanned_pdf, ocr_enabled=True
        )
        
        # OCR should extract text from scanned documents
        assert len(result_with_ocr["content"]) > len(result_no_ocr["content"])
        assert result_with_ocr["processing_options"]["ocr_enabled"] is True
```

### Error Scenario Testing
[Source: docs/architecture.md#Error-Handling]

**Error Handling Integration Tests:**
```python
@pytest.mark.integration
class TestErrorScenarios:
    def test_oversized_file_rejection(self, client):
        """Test rejection of oversized files."""
        # Create 11MB file
        oversized_content = b"x" * (11 * 1024 * 1024)
        
        response = client.post(
            "/api/upload",
            files={"file": ("oversized.pdf", oversized_content, "application/pdf")}
        )
        
        assert response.status_code == 400
        error_data = response.json()
        assert "FILE_TOO_LARGE" in error_data["error"]["code"]
        assert "maximum size is 10MB" in error_data["error"]["message"]
    
    def test_unsupported_format_rejection(self, client):
        """Test rejection of unsupported file formats."""
        response = client.post(
            "/api/upload",
            files={"file": ("test.txt", b"content", "text/plain")}
        )
        
        assert response.status_code == 400
        error_data = response.json()
        assert "UNSUPPORTED_FORMAT" in error_data["error"]["code"]
        assert "supported formats: PDF, DOCX, PPTX, XLSX" in error_data["error"]["message"]
    
    def test_corrupted_file_handling(self, client):
        """Test handling of corrupted files."""
        with open("test_fixtures/corrupted_document.pdf", "rb") as f:
            response = client.post(
                "/api/upload",
                files={"file": ("corrupted.pdf", f, "application/pdf")}
            )
        
        # Upload should succeed, but processing should fail
        assert response.status_code == 200
        document_id = response.json()["id"]
        
        # Monitor processing until failure
        max_wait_time = 120
        start_time = time.time()
        
        while time.time() - start_time < max_wait_time:
            status_response = client.get(f"/api/status/{document_id}")
            status = status_response.json()
            
            if status["status"] == "failed":
                assert "corrupted" in status.get("error_message", "").lower()
                break
            elif status["status"] == "complete":
                pytest.fail("Corrupted file should not process successfully")
            
            time.sleep(2)
        else:
            pytest.fail("Corrupted file processing should fail quickly")
```

### Performance Benchmarking
[Source: docs/architecture.md#Performance-Standards]

**Performance Benchmark Tests:**
```python
@pytest.mark.integration
class TestPerformanceBenchmarks:
    async def test_processing_time_percentiles(self, client, test_db):
        """Measure processing time percentiles across different file types."""
        test_files = [
            ("simple.pdf", "PDF", "small"),
            ("complex.pdf", "PDF", "large"),
            ("simple.docx", "DOCX", "small"),
            ("complex.docx", "DOCX", "large"),
            ("simple.pptx", "PPTX", "small"),
            ("complex.pptx", "PPTX", "large"),
            ("simple.xlsx", "XLSX", "small"),
            ("complex.xlsx", "XLSX", "large")
        ]
        
        processing_times = []
        
        for filename, file_type, size_category in test_files:
            for mode in ["fast", "quality"]:
                time_taken = await self._measure_processing_time(
                    client, filename, processing_mode=mode
                )
                
                processing_times.append({
                    "file": filename,
                    "type": file_type,
                    "size": size_category,
                    "mode": mode,
                    "time": time_taken
                })
        
        # Calculate percentiles
        times = [result["time"] for result in processing_times]
        p50 = np.percentile(times, 50)
        p95 = np.percentile(times, 95)
        p99 = np.percentile(times, 99)
        
        # Document results
        self._document_performance_results({
            "p50": p50,
            "p95": p95,
            "p99": p99,
            "detailed_results": processing_times
        })
        
        # Verify performance thresholds
        assert p50 < 60  # 50th percentile should be under 60 seconds
        assert p95 < 120  # 95th percentile should be under 2 minutes
```

### Staging Environment Testing
[Source: docs/architecture.md#Deployment-Strategy]

**Staging Environment E2E Tests:**
```python
@pytest.mark.staging
class TestStagingE2E:
    @pytest.fixture
    def staging_client(self):
        """Create client configured for staging environment."""
        return TestClient(
            app,
            base_url="https://staging.docling-app.com"
        )
    
    async def test_real_workshop_documents(self, staging_client):
        """Test with real workshop sample documents."""
        workshop_documents = [
            "complex_financial_report.pdf",  # Complex tables and charts
            "multi_section_presentation.pptx",  # Multiple slides with varied content
            "large_dataset_spreadsheet.xlsx",  # Multiple sheets with formulas
            "scanned_contract.pdf",  # Scanned document requiring OCR
            "mixed_content_document.docx"  # Text, images, and tables
        ]
        
        results = []
        
        for doc in workshop_documents:
            result = await self._test_document_workflow(staging_client, doc)
            results.append(result)
        
        # Calculate success rate
        successful_tests = sum(1 for r in results if r["success"])
        success_rate = successful_tests / len(results)
        
        assert success_rate >= 0.95  # 95% success rate required
        
        # Document findings for Epic 3
        self._document_staging_results(results)
```

### Test Results Documentation
[Source: docs/architecture.md#Documentation-Standards]

**Test Results Reporting:**
```python
class TestResultsDocumentation:
    def generate_integration_test_report(self, test_results: dict) -> str:
        """Generate comprehensive integration test report."""
        report = {
            "summary": {
                "total_tests": len(test_results["tests"]),
                "passed": test_results["passed"],
                "failed": test_results["failed"],
                "success_rate": test_results["success_rate"],
                "execution_time": test_results["execution_time"]
            },
            "performance": {
                "processing_times": {
                    "p50": test_results["performance"]["p50"],
                    "p95": test_results["performance"]["p95"],
                    "p99": test_results["performance"]["p99"]
                },
                "file_type_performance": test_results["performance"]["by_file_type"]
            },
            "error_scenarios": {
                "handled_errors": test_results["errors"]["handled"],
                "unhandled_errors": test_results["errors"]["unhandled"],
                "error_recovery_rate": test_results["errors"]["recovery_rate"]
            },
            "edge_cases": test_results["edge_cases"],
            "recommendations": test_results["recommendations"]
        }
        
        # Generate markdown report
        return self._format_report_as_markdown(report)
```

### File Locations
[Source: docs/architecture/source-tree.md]

**Integration Test Files:**
- `backend/tests/test_integration/test_complete_workflow.py` - End-to-end workflow tests
- `backend/tests/test_integration/test_processing_options_integration.py` - Processing options validation
- `backend/tests/test_integration/test_error_scenarios.py` - Error scenario testing
- `backend/tests/test_integration/test_timeout_scenarios.py` - Timeout handling tests
- `backend/tests/test_integration/test_storage_failures.py` - Storage failure testing
- `backend/tests/test_integration/test_staging_e2e.py` - Staging environment tests
- `backend/tests/test_integration/test_performance_benchmarks.py` - Performance benchmarking
- `backend/tests/test_integration/test_results_documentation.py` - Test results reporting

**Test Fixtures:**
- `backend/tests/fixtures/integration/` - Integration test files
- `backend/tests/fixtures/integration/simple_document.pdf` - Simple PDF for testing
- `backend/tests/fixtures/integration/complex_document.pdf` - Complex PDF with tables
- `backend/tests/fixtures/integration/scanned_document.pdf` - Scanned PDF for OCR testing
- `backend/tests/fixtures/integration/oversized_document.pdf` - 11MB+ file for size testing
- `backend/tests/fixtures/integration/corrupted_document.pdf` - Intentionally corrupted file

**Test Configuration:**
- `backend/tests/conftest.py` - Integration test configuration and fixtures
- `backend/tests/test_integration/__init__.py` - Integration test utilities

### Success Criteria Validation

**Story Complete When:**
1. ✅ Complete workflow integration tests for all file types
2. ✅ Processing options validation with performance comparison
3. ✅ Error scenario testing with user-friendly error messages
4. ✅ Timeout scenario testing with proper failure handling
5. ✅ Storage failure testing with graceful degradation
6. ✅ Staging environment end-to-end testing with real documents
7. ✅ Comprehensive test results documentation with recommendations
8. ✅ Performance benchmarking with percentile measurements
9. ✅ Cross-browser compatibility testing
10. ✅ Automated test execution in CI/CD pipeline

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-06 | 1.0 | Initial story draft for integration testing and error scenario validation | Scrum Master |

## Dev Agent Record

### Agent Model Used

claude-sonnet-4-5-20250929

### Debug Log References

N/A - Story in draft phase

### Completion Notes

N/A - Story not yet implemented

### File List

N/A - Story not yet implemented

## QA Results

<!-- To be populated by QA agent -->